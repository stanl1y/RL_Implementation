### Train agent with TDIL reward + IRL reward
```
python main.py --main_stage neighborhood_il --main_task neighborhood_sac --env <YOUR ENV> --wrapper basic --episode 20000 --data_name <EXPERT DATA PATH> --no_bc --beta 0.9 --use_discriminator
```


<div id="top"></div>
<br />
<div align="center">
  <h1 align="center">TDIL (Imitation Learning via Transition Discriminator)</h1>
</div>

## Introduction of this project
In the realm of imitation learning (IL), a particularly challenging but practical paradigm is the single-demonstration IL, where only a single demonstration is available to the learner. This situation holds practical relevance in real-world scenarios such as autonomous vehicle training, where repeated demonstrations, continuous human supervision, or reward function formulation can be strenuous and expensive. This study introduces a novel algorithm specifically tailored for this setting. We propose a transition discriminator to estimate the one-step transition probability, serving as an indicator of the likelihood of reaching the expert states. Our transition discriminator, unlike conventional discriminators, processes pairs of agent and expert states, generating reward signals that guide a reinforcement learning (RL) agent towards the immediate vicinity of expert states. The method demonstrates significant correlation with inaccessible ground truth reward signals, and offers a practical solution for blind model selection. We validate our proposed methodology through comprehensive experiments on five challenging MuJoCo benchmarks, in which the proposed methodology delivers exceptional performance, matching expert-level results and outperforming the existing baseline algorithms.

## Preparation
* This project can be executed with Python 3.7.7.
* Install the required packages by running the following command:
```bash
pip install -r requirements.txt
```

## Expert data
### Path to expert data
The expert data is stored in the folder `saved_expert_transition/`. The expert data is generated by a trained SAC agent and is stored in dictionary format with the following keys: "states", "actions", "rewards", "next_states", "dones". The value of each keys contains a numpy array.

### Score of expert data
* Hopper-v3: 4114
* Walker-v3: 6123
* Ant-v3: 6561
* HalfCheetah-v3: 15251
* Humanoid-v3: 5855


## Scripts
### TDIL+BC
* Hopper-v3:
    ```
    python main.py --main_stage neighborhood_il --main_task neighborhood_sac --env Hopper-v3 --wrapper basic --episode 5000 --data_name sac/episode_num1
    ``` 
    * Fix alpha:
        ```
        add
        --no_update_alpha --log_alpha_init -4.6
        ```

* Walker-v3:
    ```
    python main.py --main_stage neighborhood_il --main_task neighborhood_sac --env Walker2d-v3 --wrapper basic --episode 5000 --data_name sac/episode_num1
    ```
    * Fix alpha:
        ```
        add
        --no_update_alpha --log_alpha_init -1.2
        ```

* Ant-v3:
    ```
    python main.py --main_stage neighborhood_il --main_task neighborhood_sac --env Ant-v3 --wrapper basic --episode 10000 --data_name sac/episode_num1 --terminate_when_unhealthy
    ```
    * Fix alpha:
        ```
        add
        --no_update_alpha --log_alpha_init -1.9
        ```

* HalfCheetah-v3:
    ```
    python main.py --main_stage neighborhood_il --main_task neighborhood_sac --env HalfCheetah-v3 --wrapper basic --episode 5000 --data_name sac/episode_num1
    ``` 
    * Fix alpha:
        ```
        add
        --no_update_alpha --log_alpha_init 0.4
        ```

* Humanoid-v3:
    ```
    python main.py --main_stage neighborhood_il --main_task neighborhood_sac --env Humanoid-v3 --wrapper basic --episode 25000 --data_name sac/episode_num1 --terminate_when_unhealthy
    ``` 
    * Fix alpha:
        ```
        add
        --no_update_alpha --log_alpha_init -0.6
        ```

### TDIL+IRL
* Hopper-v3:
    ```
    python main.py --main_stage neighborhood_il --main_task neighborhood_sac --env Hopper-v3 --wrapper basic --episode 5000 --data_name sac/episode_num1  --no_bc --beta 0.9 --use_discriminator
    ``` 
* Walker-v3:
    ```
    python main.py --main_stage neighborhood_il --main_task neighborhood_sac --env Walker2d-v3 --wrapper basic --episode 5000 --data_name sac/episode_num1  --no_bc --beta 0.9 --use_discriminator
    ```
* Ant-v3:
    ```
    python main.py --main_stage neighborhood_il --main_task neighborhood_sac --env Ant-v3 --wrapper basic --episode 10000 --data_name sac/episode_num1 --terminate_when_unhealthy  --no_bc --beta 0.9 --use_discriminator
    ```
* HalfCheetah-v3:
    ```
    python main.py --main_stage neighborhood_il --main_task neighborhood_sac --env HalfCheetah-v3 --wrapper basic --episode 5000 --data_name sac/episode_num1  --no_bc --beta 0.9 --use_discriminator
    ``` 
* Humanoid-v3:
    ```
    python main.py --main_stage neighborhood_il --main_task neighborhood_sac --env Humanoid-v3 --wrapper basic --episode 25000 --data_name sac/episode_num1 --terminate_when_unhealthy  --no_bc --beta 0.9 --use_discriminator
    ``` 

### Run without the BC loss
Add the following flag:
```
--no_bc
```

### Run without hard negative samples
Add the following flag:
```
--no_hard_negative_sampling
```